{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP\n",
    "To begin, we will be generating data consistent with ordinary least squares:\n",
    "\n",
    "$$ ùê≤= x\\beta + \\epsilon $$\n",
    "where \n",
    "- ùê≤ is an ùëÅ√ó1 vector of dependent variables, \n",
    "- ùê± is an ùëÅ√óùêæ vector of dependent variables, \n",
    "- $\\beta$ are the ùêæ√ó1 parameters we are trying to estimate, \n",
    "- $\\epsilon$ is an ùëÅ√ó1 vector of iid mean zero homoskedastic unobservables having variance $\\sigma^2$.\n",
    "\n",
    "Here we generate data for ùëÅ=500\n",
    " and ùêæ=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tensorflow data type\n",
    "dtype = tf.float32\n",
    "\n",
    "##\n",
    "## simple OLS Data Generation Process\n",
    "##\n",
    "# True beta\n",
    "b = np.array([10, -1])\n",
    "N = 500\n",
    "# True error std deviation\n",
    "sigma_e = 1\n",
    "\n",
    "x = np.c_[np.ones(N), np.random.randn(N)]\n",
    "y = x.dot(b) + sigma_e * np.random.randn(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run OLS with the DGP to be compared with MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        estimate   std err\n",
      "b0     10.044457  0.044706\n",
      "b1     -0.940323  0.045264\n",
      "sigma   0.997629       NaN\n"
     ]
    }
   ],
   "source": [
    "# estimate parameter vector, errors, sd of errors, and se of parameters\n",
    "bols = np.linalg.inv(x.T.dot(x)).dot(x.T.dot(y))\n",
    "err = y - x.dot(bols)\n",
    "sigma_ols = np.sqrt(err.dot(err)/(x.shape[0] - x.shape[1]))\n",
    "se = np.sqrt(err.dot(err)/(x.shape[0] - x.shape[1]) * np.diagonal(np.linalg.inv(x.T.dot(x))))\n",
    "# put results together for easy viewing\n",
    "ols_parms = np.r_[bols, sigma_ols]\n",
    "ols_se = np.r_[se, np.nan]\n",
    "print(pd.DataFrame(np.c_[ols_parms, ols_se],columns=['estimate', 'std err'],\n",
    "      index=['b0', 'b1', 'sigma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Data into TF tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<dtype: 'float32'>\n",
      "(500, 2)\n",
      "[[ 1.         -0.8671768 ]\n",
      " [ 1.         -0.335465  ]\n",
      " [ 1.         -0.32457274]\n",
      " [ 1.         -2.2033095 ]\n",
      " [ 1.          0.37562507]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(x, dtype=dtype)\n",
    "Y = tf.constant(y, dtype=dtype)\n",
    "print(type(X))\n",
    "print(X.dtype)\n",
    "print(X.shape)\n",
    "# extract the first 5 tensors into numpy array\n",
    "print(X.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Likelihood in TF\n",
    "Next let's define the log-likelihood function using tensorflow primitives. It goes without saying that we could use tensorflow_probability.distributions.normal for this problem.\n",
    "\n",
    "$$ LogL =\\sum_{i=1}^N log (p(y_i|x_i,b,s)) = \\sum_{i=1}^N log (\\frac{1}{\\sqrt{2\\pi s^2}}e^{\\frac{-(y_i-x_ib)^2}{2s^2}})$$\n",
    "\n",
    "$$ LogL =-\\frac{N}{2}log(2\\pi s^2)-\\sum_{i=1}^N\\frac{1}{2s^2(y_i-x_ib)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = tf.constant(np.pi, dtype=dtype)\n",
    "\n",
    "@tf.function\n",
    "def ols_loglike(beta, sigma):\n",
    "    \"\"\"\n",
    "    This function defines the negative log likelihood function.\n",
    "    Argu:\n",
    "        beta (list): the list of floats as the coefficients of X\n",
    "        sigma (float): float number as the standard deviation of residuals\n",
    "    Return:\n",
    "        ll (EagerTensor): calculated negative log likelihood\n",
    "    \"\"\"\n",
    "    # xb (mu_i for each observation)\n",
    "    mu = tf.linalg.matvec(X, beta)\n",
    "    # this is normal pdf logged and summed over all observations\n",
    "    ll = - (X.shape[0]/2.)*tf.math.log(2.*pi*sigma**2) -\\\n",
    "\t    (1./(2.*sigma**2.))*tf.math.reduce_sum((Y-mu)**2., axis=-1)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 08:45:36.570711: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-12-22 08:45:36.582962: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-22626.936>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_loglike(beta = [1., 1.], sigma=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Differenciation\n",
    "Auto differentiation is possible since tensorflow in the background has defined the model graph given data for finding $\\frac{\\partial{LogL}}{\\partial{\\beta}}$ and $\\frac{\\partial{LogL}}{\\partial{\\alpha}}$. Evaluating the function and derivative at $\\beta =[1., 1.]$\n",
    " and $\\sigma=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Value:  tf.Tensor(-22626.936, shape=(), dtype=float32)\n",
      "Gradients on beta: \n",
      " [<tf.Tensor: shape=(), dtype=float32, numpy=4583.1455>, <tf.Tensor: shape=(), dtype=float32, numpy=-1230.3246>]\n",
      "Gradient on sigma: \n",
      " tf.Tensor(43834.934, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "[funval, grads] = tfp.math.value_and_gradient(ols_loglike, [[1., 1.], 1.])\n",
    "print(\"Function Value: \", funval)\n",
    "print('Gradients on beta: \\n', grads[0])\n",
    "print('Gradient on sigma: \\n', grads[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation in Tensorflow\n",
    "For finding the maximum likelihood estimate, we'll be minimizing the negative log-likelihood since tensorflow has an optimization library based on minimization (rather than maximizing the log-likelihood). Since we will be using a minimizer, we need to construct a negative log-likelihood function in tensorflow. Additionally, we need all of the parameters (the $\\beta$'s and $\\sigma$) to enter as a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to calculate gradiants\n",
    "# objective function for minimization\n",
    "@tf.function\n",
    "def neg_ols_loglike(param_vec):\n",
    "   \"\"\"\n",
    "   The function that uses gradiants descent to minimize the negative log likelihood function.\n",
    "   Argu:\n",
    "      param_vec: vector of parameters as the initial value of [beta,sigma]\n",
    "   Returns:\n",
    "      Maximized Log Likelihood.\n",
    "   \"\"\"\n",
    "   beta_split, sigma_split = tf.split(param_vec, [2,1], axis=0)\n",
    "   # need to take these back down to vectors and scalars:\n",
    "   beta_split = tf.reshape(beta_split,(2,))\n",
    "   sigma_split = tf.reshape(sigma_split,())\n",
    "   # xb (mu_i for each observation)\n",
    "   mu = tf.linalg.matvec(X, beta_split)\n",
    "   # this is normal pdf logged and summed over all observations\n",
    "   ll =  -(X.shape[0]/2.)*tf.math.log(2.*pi*sigma_split**2.) -\\\n",
    "\t   (1./(2.*sigma_split**2.))*tf.math.reduce_sum((Y-mu)**2., axis=-1)\n",
    "   return -1*ll\n",
    "\n",
    "# return function value and gradients\n",
    "@tf.function\n",
    "def neg_like_and_gradient(parms):\n",
    "    return tfp.math.value_and_gradient(neg_ols_loglike, parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value:  tf.Tensor(22626.936, shape=(), dtype=float32)\n",
      "Gradients:  tf.Tensor([ -4583.1455   1230.3246 -43834.934 ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# using the same values as above\n",
    "testval = tf.constant([1., 1., 1.], shape=(3))\n",
    "out = neg_like_and_gradient(parms=testval)\n",
    "print(\"Function value: \", out[0])\n",
    "print(\"Gradients: \", out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note that may possibly save someone alot of time: for the optimizer code below to work, the function values and gradients returned by neg_like_and_gradient must be the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        estimate   std err\n",
      "b0     10.044456  0.043704\n",
      "b1     -0.940323  0.045031\n",
      "sigma   0.995632  0.031042\n"
     ]
    }
   ],
   "source": [
    "# set some naiive starting values\n",
    "start = [0., 0., 1.]\n",
    "\n",
    "# optimization\n",
    "optim_results = tfp.optimizer.bfgs_minimize(\n",
    "    neg_like_and_gradient, start, tolerance=1e-8)\n",
    "\n",
    "# organize results\n",
    "est_params = optim_results.position.numpy()\n",
    "est_serr = np.sqrt(np.diagonal(optim_results.inverse_hessian_estimate.numpy()))\n",
    "print(pd.DataFrame(np.c_[est_params, est_serr],columns=['estimate', 'std err'],\n",
    "      index=['b0', 'b1', 'sigma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the maximum likelihood solution by inspecting the returned gradient information in optim_results, or by calculating directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value:  tf.Tensor(707.2802, shape=(), dtype=float32)\n",
      "Gradients:  tf.Tensor([-2.0015240e-04  8.4400177e-05  6.1035156e-05], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "out = neg_like_and_gradient(est_params)\n",
    "print(\"Function value: \", out[0])\n",
    "print(\"Gradients: \", out[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
